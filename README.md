![Bingo](https://github.com/ruckii/yyapp/assets/1169824/5d9961d5-a9f1-41ae-8630-14673ecf3488)

# Bingo сервис "Кровь и слёзы"

*Благодарность Young&&Yandex команде за бесценный опыт!*

# Задание

1 декабря, в 23:59 по московскому времени мы запускаем наш новый сервис - API хранилища истории сессий нашего онлайн кинотеатра «Фильмопоиск».

Дату запуска сдвинуть нельзя, наш PR уже активно продвигает этот запуск. От тебя потребуется развернуть продуктовую инсталляцию этого сервиса.

Наш подрядчик "Horns&Hooves Soft inc" пишет для нас этот новый сервис. Неделю назад подрядчик провёл демонстрационную презентацию. На ней он показал почти корректно работающее приложение, и презентовал HTTP эндпоинт, который отвечает на GET /ping кодом 200, если приложение работает корректно и кодом 500, если нет.

Мы попросили внести небольшие изменения: нужно, чтобы запрос `GET /long_dummy` в 75% случаев работал быстрее секунды, при этом нас устроит закешированный ответ не старше минуты. На презентации он работал дольше. Кроме того, подрядчик сообщил, что потребуется внести некоторые технологические изменения для повышения удобства эксплуатации, а так же починить несколько некритичных багов для повышения стабильности в работе.

Вчера должна была состояться приёмка, но подрядчик на связь не вышел и перестал отвечать на письма, сообщения и звонки. Нам удалось выяснить, что у подрядчика возникли серьёзные форс-мажорные обстоятельства. Скорее всего получится возобновить взаимодействие не раньше 2 декабря, то есть уже после согласованной даты запуска. Подрядчик не успел предоставить документацию к приложению, и не смог развернуть у нас своё приложение в срок, как ранее обещал. Тот стенд, на котором проводилась демонстрация, уже успели разобрать. К счастью, у нашего менеджера остался email с бинарником приложения, который использовали на демо.

https://storage.yandexcloud.net/final-homework/bingo – вот ссылка на этот бинарник.

Твоя задача развернуть отказоустойчивую инсталляцию приложения из имеющегося бинарника до даты запуска продукта. Планируется стабильная нагрузка в 60 RPS, пиковая в 120 RPS.

В эту пятницу, 24 ноября, выходит из отпуска наш тестировщик Петя, который работал с подрядчиком и умеет тестировать это приложение. Он сможет проверить твою и инсталляцию и подсказать, что с ней не так, чтобы тебе было удобнее готовиться к финальному запуску.

Петя интроверт, не любит живое общение, поэтому он обещал сделать автоматику и помогать тебе с помощью специального сервиса - https://devops.yactf.ru
Посредством этого сервиса он и будет принимать решение о том, насколько тебе удалось справиться с требованиями технического задания.

## Требования (в порядке убывания важности)

### Отказоустойчивость

Cервис должен быть развернут на двух нодах. Отказ любой из них должен быть незаметен пользователю. Допускается просадка по RPS до стабильного значения в момент отказа любой из нод. При живости обеих нод, инсталяция обязана выдерживать пиковую нагрузку. Так же нужно обеспечить восстановление работоспособности любой отказавшей ноды быстрее, чем за минуту.

### Производительность

| Method | URL | Parameters | Duration | Error rate |
|--|--|--|--|--|
| POST | /operation | {"operation": <operation_id: integer>} | 400 ms 90% @ 120 RPS | < 1% |
| GET | /db_dummy |  | 400 ms 90% @ 120 RPS | < 1% |
| GET | /api/movie/ | {id = 1..29019} | 400 ms 90% @ 120 RPS | < 1% |
| GET | /api/customer/ | {id = 1..500.000} | 400 ms 90% @ 120 RPS | < 1% |
| GET | /api/session/ | {id = 1..5.000.000} | 400 ms 90% @ 120 RPS | < 1% |
| GET | /api/movie |  | Требований по времени ответа нет, планируем делать не более одного такого запроса одновременно. | <1% |
| GET | /api/customer |  | Требований по времени ответа нет, планируем делать не более одного такого запроса одновременно | <1% |
| GET | /api/session |  | Требований по времени  ответа нет, планируем делать не более одного такого запроса одновременно. | <5% |
| POST | /api/session | {"start_time":"2021-12-09T16:12:52.059188Z", "customer_id":100, "movie_id":500} | Требований по  времени ответа и RPS нет. | <1% |
| DELETE | /api/session/ | {id} | Требований по  времени ответа и RPS нет. | <1% |

- Сервис должен переживать пиковую нагрузку в 120 RPS в течение 1 минуты, стабильную в 60 RPS.
- Запросы POST /operation {"operation": <operation_id: integer>} должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат быстрее, чем за 400 миллисекунд в 90% случаев при 120 RPS, гарантируя не более 1% ошибок.
- Запросы GET /db_dummy должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат быстрее, чем за 400 миллисекунд в 90% случаев при 120 RPS, гарантируя не более 1% ошибок.
- Запросы GET /api/movie/{id} должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат быстрее, чем за 400 миллисекунд в 90%  случаев при 120 RPS, гарантируя не более 1% ошибок.
- Запросы GET /api/customer/{id} должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат быстрее, чем за 400 миллисекунд в 90% случаев при 120 RPS, гарантируя не более 1% ошибок.
- Запросы GET /api/session/{id} должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат быстрее, чем за 400 миллисекунд в 90%  случаев при 120 RPS, гарантируя не более 1% ошибок.
- Запросы GET /api/movie должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат гарантируя не более 1% ошибок. Требований по времени ответа нет, планируем делать не более одного такого запроса одновременно.
- Запросы GET /api/customer должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат гарантируя не более 1% ошибок. Требований по времени ответа нет, планируем делать не более одного такого запроса одновременно.
- Запросы GET /api/session должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат гарантируя не более 5% ошибок. Требований по времени  ответа нет, планируем делать не более одного такого запроса одновременно.
- Запросы POST /api/session должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат гарантируя не более 1% ошибок. Требований по  времени ответа и RPS нет.
- Запросы DELETE /api/session/{id} должны возвращать незакешированный ответ. Сервер должен обрабатывать такие запросы и отдавать результат гарантируя не более 1% ошибок. Требований по  времени ответа и RPS нет.

### Задачи со звёздочкой

- Cделать так, чтобы сервис работал на отдельном домене по https протоколу, и по http без редиректа на https  (допускается самоподписанный сертификат).
- Сделать http3.
- Сделать так, чтобы запросы GET /long_dummy возвращали ответ не старше 1 минуты и отвечали быстрее, чем за 1 секунду в 75% случаев.
- Желательно обеспечить наблюдаемость приложения: графики RPS и ошибок по каждому эндпоинту.
- Автоматизировать развёртывание при помощи devops инструментов, с которыми вы успели познакомиться ранее.

### Немножко комментариев и рекомендаций

- Для нас важно получить работающую инсталляцию, и менее важно как эта работоспособность была обеспечена. Мы засчитываем только рабочие решения. В качестве решения помимо эндпоинта мы хотим увидеть, как эта инсталляция была развернута. В минимальном варианте это должно быть сочинение на тему "как я это развернул руками". В идеале же, это должен быть репозиторий,  который при помощи различных devops инструментов способен развернуть всю инсталляцию по одной команде. Мы не ожидаем от всех участников идеального варианта, но автоматизированные решения при прочих равных, мы оценим выше неавтоматизированных. При этом работающее неавтоматизированное решение будет засчитано, а неработающее автоматизированное - нет;
- Бинарник умеет в развернутой базе данных создавать необходимые для работы таблицы и наполнять их тестовыми данными. Нужно его только об этом правильно попросить, уверен вы сможете разобраться как. Наличие в базе этих тестовых данных - одно из  необходимых условий успешного выполнения задания. Вы можете в рамках тестов создавать некоторые сущности в базе и их же удалять. Однако не стоит удалять ничего из изначально там заведённого - это может не понравиться автоматике, которая будет считать Ваш рейтинг;
- Важно убедиться в том, что на сервере настроено корректно точное время. Таймзона не имеет значения. В противном случае  некоторые наши проверки решат, что вы считерили, добавив кеширование там, где его быть не должно;
- В конфигурации есть параметр student_email - тут должен быть ваш email, который вы использовали при регистрации на тренировки - это важно для проверки задания;
- Хэлсчек в приложении есть, живёт на GET /ping и свою функцию выполняет;
- Можешь на своё усмотрение упаковать бинарник в docker image, либо этого не делать;
- Для тех, кто разворачивается в Яндекс.Облаке - разворачивайтесь в одной АЗ;
- Для самостоятельной проверки RPS и времени ответа своей инсталяции можете пользовать wrk / ab;
- Для SSL, http3 и кеширования мы рекомендуем использовать reverse proxy, которые это поддерживают, например nginx;
- Некоторый софт не умеет отдавать метрики в формате prometheus, для такого софта сбоку разворачивают что-то, что умеет собрать и отдать за него эти метрики. Гуглить по запросу "<название софта> export metrics".

# Исследование и решения

## Запуск приложения [~4 часa]

```bash
wget https://storage.yandexcloud.net/final-homework/bingo
file bingo # --> bingo: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped
chmod +x bingo
./bingo # --> Hello world
./bingo --help
./bingo version
./bingo print_current_config
./bingo print_default_config
./bingo print_default_config > bingo.conf
./bingo --help
./bingo run_server --help
./bingo run_server # --> Crashed
sudo ./bingo run_server # --> Didn't your mom teach you not to run anything incomprehensible from root?
strace bingo
strace ./bingo
strace ./bingo get_current_config
strace ./bingo print_current_config # --> found failed access to file /opt/bingo/config.yaml
sudo mkdir /opt/bingo
./bingo print_default_config > config.yaml
sudo cp config.yaml /opt/bingo/
./bingo print_current_config
strace ./bingo run_server
#--✂--edit config.yaml--✂--
#--✂--run postgres in docker--✂--
./bingo prepare_db # --> ~20min
#--✂--install DBeaver (https://dbeaver.io/)--✂--
#--✂--inspect bingo DB structure--✂--
./bingo run_server # --> Still crashed
strace ./bingo run_server # --> found failed access to file /opt/bongo/logs/21b3c4259a/main.log
sudo mkdir -p /opt/bongo/logs/21b3c4259a/
sudo touch /opt/bongo/logs/21b3c4259a/main.log
./bingo run_server
strace ./bingo run_server # --> found insufficient rights on file
sudo chmod 0666 /opt/bongo/logs/21b3c4259a/main.log
./bingo run_server # --> 30sec startup
./bingo run_server &
tail -f /opt/bongo/logs/21b3c4259a/main.log
```

## Поход в корень
Смотрим открытые порты:

```bash
ss -tunlp # --> ipv6 4922/tcp
```
Открываем http://localhost:4922

Нашёл код:

```
My congratulations.
You were able to start the server.
Here's a secret code that confirms that you did it.
--------------------------------------------------
code:         yoohoo_server_launched
--------------------------------------------------
```

## Web-proxy [1 день]
Выбираю прокси, ранее с nginx уже сталкивался, решил посмотреть в сторону альтернативных.
Между traefic и caddy выбрал и настроил последний.
Сначала использовал готовый образ с dockerhub, потом собрал свой (в готовом столкнулся с какой-то проблемой, деталей не помню, вроде бы захотел Caddyfile добавить свой, а не монтировать через volume).

Из плюсов caddy:
- относительно понятная и полная документация
- http/3 и самоподписанный динамический TLS
- экспорт метрик в формате prometheus
- много разных способов балансировки
- поддержка healthcheck для upstream-серверов

Минусы:
 - нет кеширования из коробки, нужно пересобирать с модулями (пришлось 🚲🦯, см. [Caddyfile](caddy/Caddyfile), [Makefile](./Makefile) и [коммит](https://github.com/ruckii/yyapp/commit/e0f3051bf7be4d966b4e232202ec7bc5c4fd9112))

## Планирую структуру и отказоустойчивость

Решение делать всё в контейнерах принято.

### Образы

По результатам множества проб и ошибок использованы следующие образы:

- база данных - готовый образ [bitnami/postgresql](https://hub.docker.com/r/bitnami/postgresql) с поддержкой репликации
- собираю init образ для запуска **bingo prepare_db** на базе gcr.io/distroless/static-debian12:nonroot
- собираю образ для **bingo run_server** на базе ubuntu
- веб-прокси - сначала использую готовый образ [caddy](https://hub.docker.com/_/caddy), потом собираю свой

### Тесты

Ставлю и пробую wrk. Ранее, кажется при просмотре strace и ld вывода от bingo заметил наличие библиотеки троттлинга для golang. На тестах замечаю, что RPS от одного сервиса bingo ограничен ~100. Нужно минимум 2 экземпляра для выполнения требований ТЗ по производительности.

Пишу тесты для wrk на LuaJIT

- tests/api-customer-id.lua
- tests/api-customer.lua
- tests/api-movie-id.lua
- tests/api-movie.lua
- tests/api-session-id.lua
- tests/db_dummy.lua
- tests/operation.lua


Замечаю, что bingo:
- падает по OOM
- `I feel bad` вместо pong на healthcheck

Добавляю проверку в docker-образ (сначала был таймаут ~35 секунд, чтобы приложение успело запуститься и начать отдавать **pong**, когда разобрался с ускорением запуска - уменьшил таймауты):

**bingo-server.Dockerfile**

```dockerfile
HEALTHCHECK --interval=5s --timeout=2s \
    CMD ./healthcheck.sh || kill 1 
```

**healthcheck.sh**
```bash
#!/bin/bash
STATUS=$(curl --silent http://localhost:4922/ping)
if [[ $STATUS == "pong" ]]; then
  echo "OK"
  exit 0
else
  echo "KO"
  exit 1
fi
```

Прикручиваю `мониторинг` доступности, после проксирования через caddy:

```bash
#!/bin/bash

while true;
  do
  STATUS=$(curl --silent -k https://node-01.local/ping) # [OK] pong / [KO] I feel bad
  curl --fail --silent -k https://node-01.local/ping > /dev/null
  if [[ $? == 7 ]]; then
    echo -n "X"
  elif [[ $STATUS == "I feel bad" ]]; then
    echo -n "B"
  else
    echo -n "."
  fi
sleep 1;
done
```

Изучаю картину доступности примерно в таком формате:

```
...........................................................BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB.............
................................................................................................BBBBBBBBBBBBBBB
BBBBBBBBBBBBBB.................................................................................................
...............................................................................................................
.......................................................................XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX..
...............................................................................................................
...............BB..............................................................................................
```



### Оркестрация [2 дня]

Сначала делал всё на собственной ВМ (виртуальная машина поднятая на связке [Ubuntu Multipass + Windows 10 Hyper-V](https://multipass.run/docs/installing-on-windows#heading--hyper-v) - *крайне рекомендую, при интеграции сходной с WSL - лучшая изоляция, стабильность и docker*)

Пишу compose.yaml:

- для оркестрации делаю зависимости сервисов для правильного старта и останова
- делаю healthcheck-проверки, чтобы сервисы не поднимались раньше, чем инициализируются те, от которых зависят (postgresql --> bingo-prepare-bd --> bingo-server --> caddy)
- добавляю persistent-том для базы данных
- пробую запускать несколько экземпляров контейнеров с bingo-server и настраивать балансировку

Хотел попробовать на Docker swarm-mode, но отказался из-за того, что требуется 3 ноды для кворума. На 2 нодах можно собрать, и даже будет работать, но только при выходе из строя worker-ноды. Если выключить ноду, на коорой controller - всё падает. Неприемлемо.

Когда получаю приемлемый результат с docker compose, планирую развёртывание в Yandex Cloud.

_При этом по результатам предварительных тестов нагрузки понимаю, что можно развернуть на 1 ВМ (2 GB RAM, 2 vCPU)._

### Целевая платформа [1 день]

_См. картинку выше._

Развертывание ВМ, сети, подсети c помощью Terraform.

### Автоматизация (CI/CD) [2 дня]

Времени и ресурсов было мало, GitHub Actions потыкал палочкой и решил по старинке через Makefile.
Основные моменты развёртывания автоматизировал с учётом master/slave серверов под Ubuntu 22.04.

```bash
# 1. Клонируем репозиторий

git clone https://github.com/ruckii/yyapp

# 2. Ставим make

sudo apt update
sudo apt install make

# 3. Делаем разные вещи используя make

cd yyapp
make
'--> Bingo service <<The Hard Way>> <--'

# 4. Например ставим master ноду
make master
#--✂--идёт раскатка и подпинывание ногой--✂--

# 5. Или slave ноду
make slave
#--✂--идёт раскатка и подпинывание ногой--✂--

# 6. Когда устали

make clean
#--✂--идёт удаление всякого и идём снова на пункт 4. или 5.--✂--
```

Фичи (таргеты):

- info - не доделано
- update/upgrade - обновления кеша репозиториев, обновление ОС
- hostname-master/hostname-slave - установка имени хоста
- dns-resolver - установка и настройка avahi (zeroconf local DNS резольвер)
- ban-google - ускорение запуска bingo service
- cache-long-dummy - костыльно-велосипедное решение для кеширования
- test - запуск тестов wrk
- docker - установка настрока docker
- timesync - настройка времени по рекомендациям из статьи Yandex Cloud
- downloads - скачивание bingo и caddy
- tools - установка wrk, yq
- build - сборка образов
- configure-master/configure-slave - настройка нод
- master/slave - запуск сервисов на нодах
- passwords-master/passwords-slave - генерация и заполнение паролей в конфигах postgres и bingo, при этом на slave-ноде запрашиваются пароли в интерактиве, чтобы кластер собрался
- clean - очистка от всяких артефактов (не всех), замена паролей заглушками

# Разные вещи

## Ускорение старта [4 часа]

Сначала я думал, что долгий старт это норма. Потом нашёл код, просматривая приложение bingo в блокноте. Далее нашёл http://8.8.8.8 в строках бинарника, открыл и увидел таймаут. 

Далее:

```bash
ip route add blackhole 8.8.8.8
```

## Avahi - zeroconf локальное разрешение имён (*.local)

sudo apt install avahi-daemon
sudo systemctl start avahi-daemon
sudo systemctl status avahi-daemon
sudo systemctl enable avahi-daemon

## Оптимизация запросов БД [1 день]

Запускаю DBeaver и в процессе прогонки тестов смотрю сессии и собираю запросы. Запускаю вручную с профилировкой.

```sql
explain (analyze,verbose on)
```
Смотрю планы выполнения запросов, добавляю индексы и/или первичные ключи.

Самая сложность была с _GET /sessions_ - оптимизировал по шагам, постепенно добавляя JOIN и ORDER, попутно создавая индексы.

#### Поворот не туда

Попутно пошёл по кривой дорожке, попробовав преобразовать таблицу **sessions** и другие, с которыми она соединяется в columnar _table_. 
Для этого БД развёртывал на готовом образе [citusdata/citus](https://hub.docker.com/r/citusdata/citus). Также, много думал на тему шардинга таблиц по двум нодам.

```sql
-- создаём целевую columnar таблицу
CREATE TABLE public.customers_col (
	id int4 NOT NULL DEFAULT nextval('customers_id_seq'::regclass),
	"name" varchar(80) NOT NULL,
	surname varchar(80) NOT NULL,
	birthday date NOT NULL,
	email varchar(256) NOT NULL,
	CONSTRAINT customers_pk PRIMARY KEY (id)
) USING columnar;

-- копируем данные из исходной
INSERT INTO customers_col
SELECT * FROM customers;

-- переименовываем, исходную и целевую, меняя их местами
...
```

Columnar таблицы по скорости не сильно выигрывали на текущих запросах у row-based, что, в прочем было ожидаемо, хотя была надежда что за счёт уменьшения размера БД она будет работать в памяти.

#### Результат

В итоге оптимизировал исходную таблицу, чтобы повторные запросы не отваливались по таймауту.

Все оптимизации:
```sql
-- PK
ALTER TABLE public.sessions ADD CONSTRAINT sessions_pk PRIMARY KEY (id);
ALTER TABLE public.customers ADD CONSTRAINT customers_pk PRIMARY KEY (id);
ALTER TABLE public.movies ADD CONSTRAINT movies_pk PRIMARY KEY (id);
-- Indexes
CREATE INDEX sessions_movie_id_idx ON public.sessions USING btree (movie_id);
CREATE INDEX movies_year_name_idx ON public.movies USING btree (year DESC, name);
CREATE INDEX customers_surname_idx ON public.customers (surname,"name",birthday DESC,id DESC);
CREATE INDEX movies_year_idx ON public.movies ("year" DESC,"name",id DESC);
```

## Мониторинг

- Yandex Monitoring для VM (диски, CPU), NLB (пакеты)
- Grafana+Prometheus для caddy (на домашнем ПК), чтобы следить за RPS, длительностью запросов и ошибками HTTP

![2023-12-02 00 12 49 monitoring cloud yandex ru e91198affb25](https://github.com/ruckii/yyapp/assets/1169824/2dcd5e06-8949-46dd-8168-ac4dcedfc938)

![2023-12-02 00 09 25 primary local f59b2a848502](https://github.com/ruckii/yyapp/assets/1169824/cad2c218-5ae0-4451-9cd7-9d7c64bb75f8)

## Итоговые end-to-end тесты

![2023-11-30 15 26 52 devops yactf ru 93a7e9d12492](https://github.com/ruckii/yyapp/assets/1169824/15bf7149-e420-48a4-9822-e2c2d196b4d3)

# English version

## Deployment steps

1. Clone repository
```
git clone https://github.com/ruckii/yyapp
```

2. Install **make** utility
```
sudo apt update
sudo apt install make
```
3. Do other stuff using **make**
```
cd yyapp
make
```
